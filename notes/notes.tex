\documentclass[a4paper]{article}

%\usepackage[numbers]{natbib}
\usepackage[usenames]{color} %for colored text
\usepackage[top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}


%\bibliographystyle{naturemagurl}
%\bibliographystyle{plainnat}

\newcommand{\evec}[2]{\phi^{(#2)}_{#1}}
\newcommand{\subK}{K^{[i^{*}]}}

\title{Notes on Minimum Spanning Tree spectral decomposition and preconditioning}
\author{Jacob Stevenson}
\date{\today}

\begin{document}
\maketitle

\section{notes on NGT}
Proof of NGT, from \cite{wales.2009}.  We would like to know how the transition probabilities and waiting times
are updated when node $x$ is removed.  The node is not physically removed (the physics should remain the same), but it's
removed from the equations in the sense of real space renormalization group.  
The transition probability $P_{uv}$ is the probability that a markov chain 
transitions directly from u to v.  When we remove node $x$ we must sum over all possible paths that go through $x$.
As the nodes have self-transition probabilities, this is simply the sum over how many time steps the chain spends in $x$
\begin{equation}
P'_{uv} = P_{uv} + P_{ux} P_{xv} \sum_{m=0}^{\infty} P_{xx} = P_{uv} + \frac{ P_{ux} P_{xv} }{ 1 - P_{xx} }
\end{equation}
Computing how the waiting time is updated is only slightly more involved.  The waiting time $\tau_u$ is the average time to transition
from $u$ to one if it's neighbours.  After removing node $x$, $x$ will no longer be one of the neighbors of $u$, but we must sum
over all paths through x to a neighbour
\begin{equation}
\tau'_u = \sum_{v \ne x} \left\{ 
P_{uv} \tau_u + P_{ux} P_{xv}
\sum_{m=0}^{\infty} \left[ \tau_u + (m+1) \tau_x \right] P_{xx}^{m}
\right\}
\end{equation}

After resolving the sums ($\sum_{m=0}^{\infty} p^m = 1/(1-p)$ and $\sum_{m=0}^{\infty} m p^m = p/(1-p)^2$), this becomes
\begin{equation}
\tau'_u = \sum_{v \ne x} \left\{ 
P_{uv} \tau_u + P_{ux} P_{xv} \left[ 
\frac{\tau_u}{1-P_{xx}} + \frac{\tau_x}{(1-P_{xx})^2}
\right]
\right\}
\end{equation}
Finally we use $\sum_{v\ne x} P_{uv} = 1-P_{ux}$ and $\sum_{v\ne x} P_{xv} = 1-P_{xx}$ to conclude that
\begin{equation}
\tau'_u = \tau_u + \frac{ P_{ux} \tau_x}{1-P_{xx}}
\end{equation}

\section{pseudo-inverse (generalized inverse)}
A pseudo inverse $A^{+}$ is one which satisfied some inverse-like properties
\begin{eqnarray}
1.)&  A A^{+} A = A \\
2.)&  A^{+} A A^{+} = A^{+} \\
3.)&  (A^{+} A)^T = A^{+} A \\
4.)&  (A A^{+})^T = A A^{+}
\end{eqnarray}
If $A^{+}$ satisfies 1.) and 2.) then it is called a generalized inverse. 
If it satisfies all 4 then is is a Moore-Penrose pseudo-inverse.

Any generalized inverse can be used to determine if a system of linear equations has any solutions, and if so to give all of them. If any solutions exist for the linear system $Ax=b$, all solutions are given by
\begin{equation}
x=A^{+} b  + [I - A^{+} A ] w
\end{equation}
parametric on the arbitrary vector $w$.
Solutions exist if and only if $A^{+} b$ is a solution--that is, if and only if $A^{+} A b = b$.

\section{eigenvalues and vectors}

The eigenvalues $\lambda_n$ and eigenvectors $\phi^{(n)}_i$ satisfy the orthonormality condition
\begin{equation}
\sum_i \phi^{(n)}_i \pi_i \phi^{(m)}_i = \delta_{nm}
\end{equation}
They also satisfy the eigenvector equation
\begin{equation}
\sum_i K_{ij} \evec{i}{n}  = \lambda_n \evec{j}{n}
\end{equation}
This can be written more compactly as
\begin{align}
\Phi^{T} \Pi \Phi = I \\
K \Phi = \Phi \Lambda
\end{align}
where $\Pi$ and $\Lambda$ are diagonal matrices with $\pi_i$ and $\lambda_i$ along the diagonals
and $\Phi$ is the matrix who's columns are the eigenvectors $\Phi_{in} = \evec{i}{n}$.
The inverse of $\Phi$ is $\Phi^{T} \Pi$.
We can write K from the spectral decomposition
\begin{align}
K_{ij} &= \sum_n \lambda_n \evec{i}{n} \evec{j}{n} \pi_j \\
K &= \Phi \Lambda \Phi^T \Pi
\end{align}
The pseudo-inverse of $K$ can be written as
\begin{equation}
K^{+} = \Phi \Lambda^{-1} \Phi^T \Pi
\end{equation}
This is not the real inverse because the first eigenvector is zero.  But it does satisfy (I think)
\begin{align}
K K^{+} K &= K \\
K^{+} K K^{+} &= K^{+}
\end{align}
and maybe (but I haven't checked it)
\begin{align}
(K K^{+})^T &= K K^{+} \\
(K^{+} K)^T &= K^{+} K
\end{align}

Let's work out in detail $K K^{+}$
\begin{equation}
\sum_k K_{ik} K^{+}_{kj} = \sum_{n=1}^{N-1} \sum_{m=1}^{N-1} \sum_k
\lambda_n \evec{i}{n} \evec{k}{n} \pi_k \evec{k}{m} \evec{j}{m} \pi_j \lambda_m^{-1}
\end{equation}
The sum over $k$ leads to $\delta_{nm}$.  $\lambda_n$ and $\lambda_m^{-1}$ then cancel and we're left with
\begin{equation}
\sum_k K_{ik} K^{+}_{kj} = \sum_{n=1}^{N-1}
\evec{i}{n} \evec{j}{n} \pi_j
\end{equation}
If the sum over $n$ were from 0 this would be the identity ({\color{red} I think so, and Eric agrees, but I can't find the proof}).
So the above equation can be more simply written
\begin{equation}
\sum_k K_{ik} K^{+}_{kj} = I - \evec{i}{0} \evec{j}{0} \pi_j
\end{equation}
Assuming the normalization conditions described above $\evec{i}{0} = 1$ and this reduces to
\begin{equation}
\sum_k K_{ik} K^{+}_{kj} = I - \pi_j
\end{equation}
I checked this numerically and it works for the exact eigenvectors and works OK but not great for the approximate eigenvectors


\section{submatrix spectrum}
We ultimately want to construct a preconditioner for a submatrix of $K$ denoted $\subK{}$ which is $K$
with row and column $i^{*}$ deleted.  This matrix is used to calculate mean first passage times from all the nodes
into $i^{*}$ via
\begin{equation}
\subK{} \tau_{mfpt} = (1, \dots, 1)^T
\end{equation}
The preconditioner will be an approximation to the inverse of $\subK{}$ which we will hopefully construct from 
the spectrum of $K$.

We will proceed by altering the original problem so that the energy of the node to be removed $E_{i^{*}}$
is much less than all other energies in the problem.  The spectrum of $K$ is then computed.
We will try to show that the eigenvectors of $\subK{}$ are the same as the eigenvectors of $K$ with 
the $i^{*}$th element removed. The eigenvector corresponding to the zero eigenvalue is discarded.
We are taking the limit $E_{i^{*}} \to -\infty$, so we will define a small parameter $p = \exp{\beta E_{i^{*}}}$.
Numerically it appears that
\begin{align}
\lambda_{i} &\sim 1 \\
\evec{i^{*}}{n} &\sim p^{1/2} \\
\evec{i\ne i^{*}}{n} &\sim p^{-1/2} \text{  (except, possibly, for n=1)} \\
\pi_{i^{*}} &\sim 1 \\
\pi_{i\ne i^{*}} &\sim p \\
K_{i^{*} j} &\sim p \\
K_{i\ne i^{*}, j} &\sim 1
\end{align}

First show that they are orthonormal.
\begin{align}
\sum_{i\ne i^{*}} \evec{i}{n} \pi_i \evec{i}{m} &= \delta_{nm} - \evec{i^{*}}{n} \evec{i^{*}}{m} \pi_{i^{*}} \\
& \sim \delta_{nm} - p
\end{align}
Thus they are orthonormal for small enough p.  We now show that they satisfy the eigenvector equation
\begin{align}
\sum_{j\ne i^{*}} K_{ij} \evec{j}{n}  &=
\lambda_n \evec{i}{n} - K_{i i^{*}} \evec{i^{*}}{n} \\
& \sim \lambda_n \evec{i}{n} - p^{1/2}
\end{align}
Similarly, it can be shown that
\begin{align}
\sum_{i\ne i^{*}} 
\sum_{j\ne i^{*}} 
\evec{i}{m} \pi_{i}
K_{ij} \evec{j}{n} 
&= \lambda_n \delta_{nm} + \evec{i^{*}}{n} \evec{i^{*}}{m} \pi_{i^{*}} (K_{i^{*} i^{*}} - \lambda_n - \lambda_m)
\\
& \sim \lambda_n \evec{i}{n} + p (p - 1)
\end{align}

Thus as $p \to 0$ the eigenvalues and eigenvectors (for $n \ge 1$ and with index $i^{*}$ deleted) are the true
eigenvalues and eigenvectors of $\subK{}$.

Now we evaluate $\subK{} K^{[i_{*}]+}$ assuming that the eigenvalues and eigenvectors are exact
\begin{equation}
M_{ij} = \sum_{k} K^{+[i^{*}]}_{ik} \subK{}_{kj}  = 
\sum_{k \ne i^{*}} \sum_{n=1}^{N-1} \sum_{m=1}^{N-1} \lambda_n^{-1} \lambda_m \evec{i}{n} \evec{k}{n} \pi_k \evec{k}{m} \evec{j}{m} \pi_j
\end{equation}
The sum over $k$ is almost $\delta_{nm}$
\begin{equation}
M_{ij} = 
\sum_{n=1}^{N-1} \sum_{m=1}^{N-1} \lambda_n^{-1} \lambda_m \evec{i}{n} \evec{j}{m} \pi_j 
\left( \delta_{nm} - \evec{i^{*}}{n} \evec{i^{*}}{m} \pi_{i^{*}} \right)
\end{equation}
\begin{equation}
M_{ij} = 
\sum_{n=1}^{N-1} \evec{i}{n} \evec{j}{m} \pi_j 
-
\sum_{n=1}^{N-1} \lambda_n^{-1} \evec{i}{n} \evec{i^{*}}{n}  \pi_{i^{*}}
\sum_{m=1}^{N-1} \lambda_m 
\evec{i^{*}}{m} \evec{j}{m} \pi_j  
\end{equation}
The first term, as we found before is $I_{ij} - \pi_j$
\begin{equation}
M_{ij} = 
I_{ij} - \pi_j 
-
K^+_{ii^{*}} K_{i^{*}j}
\end{equation}
some of these becomes small as $p \to 0$
\begin{equation}
M_{ij} = 
I_{ij} - p (1 + K^+_{ii^{*}} )
\end{equation}
This equation holds only for exact eigenvalues and eigenvectors.  It's unclear how the
errors in our approximate eigenvectors play a role here.
I checked this numerically and found that it is correct for exact eigenvectors, but it fails pretty dramatically
for the approximate eigenvectors.

\subsection{apprimate eigenvalues and eigenvectors}

The approximate eigenvectors are constructed iteratively.  At step $n$ in the iteration
$s_n$ is the new sink node.  The highest barrier separating $s_n$ from a previous sink node $s_n^{old}$
(which has lower energy than $s_n$)
is called the a cutting edge $(p_n, q_n)$.
The eigenvalue corresponding to this iteration is $\lambda_n = \exp( -(E_{p_n q_n} - E_{s_n} )/ T )$.
The two groups of nodes that are created by the cutting edge are $S_1^{(n)}$ and 
$S_2^{(n)}$.  The first group contains $s_n$ and the latter group contains $s_n^{old}$.  
The eigenvector of this iteration has the form
\begin{equation}
\evec{i}{n} = a_1 \delta (i \in S_1^{(n)}) - a_2 \delta (i \in S_2^{(n)})
\end{equation}
It satisfies the normalization conditions
\begin{equation}
  0 = \sum_i \evec{i}{0} \evec{i}{n} \pi_i
  = a_1 \sum_{i \in S_1^{(n)}} \pi_i
  - a_2 \sum_{i \in S_2^{(n)}} \pi_i
  = a_1 \pi_{S_1^{(n)}}
  - a_2 \pi_{S_2^{(n)}}
\end{equation}
and
\begin{equation}
  1 = \sum_i \evec{i}{n} \evec{i}{n} \pi_i
  = a_1^2 \pi_{S_1^{(n)}}
  + a_2^2 \pi_{S_2^{(n)}}
  .
\end{equation}
This leads to the solution
\begin{equation}
  a_1 = \sqrt{ \frac{ \pi_{S_2^{(n)}}} { \pi_{S_1^{(n)}}} }
  \left( \pi_{S_1^{(n)}} + \pi_{S_2^{(n)}} \right)^{-1/2}
\end{equation}
\begin{equation}
  a_2 = \sqrt{ \frac{ \pi_{S_1^{(n)}}} { \pi_{S_2^{(n)}}} }
  \left( \pi_{S_1^{(n)}} + \pi_{S_2^{(n)}} \right)^{-1/2}
\end{equation}
or
\begin{equation}
\evec{i}{n} = 
  \sqrt{ \frac{ \pi_{S_2^{(n)}}}{
  \pi_{S_1^{(n)}} + \pi_{S_2^{(n)}} }}
  \left[
    \pi_{S_1^{(n)}}^{-1/2}
  \delta (i \in S_1^{(n)}) - 
  \frac{\pi_{S_1^{(n)}}^{1/2}}{\pi_{S_2^{(n)}}}  \delta (i \in S_2^{(n)})
  \right]
\end{equation}
If we call the common prefactor $A_n$ then the eigenvector can be written as
\begin{equation}
\evec{i}{n} = A_n
\begin{cases}
    \pi_{S_1^{(n)}}^{-1/2} & i \in S^{(n)}_1 \\
  - \frac{\pi_{S_1^{(n)}}^{1/2}}{\pi_{S_2^{(n)}}} & i \in S^{(n)}_2 \\
0 & \text{else}
\end{cases}
\end{equation}
In the scaling limit $T \to 0$ we have $\pi_{S_2^{(n)}} >> \pi_{S_1^{(n)}}$
so $A_n \to 1$.
If there is an absorbing state $i^*$ with infinitely low energy energy 
it will be in $S_2^{(n)}$.  
If we write $\pi_i = P \exp ( -E_i / T )$ and leave $P=1$ ($\pi$ unnormalized)
then even if $i^*$ is in $S_2^{(n)}$ all terms of $\evec{}{n}$ will remain finite and 
or go to zero.
If $\pi$ is normalized then $\pi_{S_2^{(n)}} \to 1$ and $\pi_{S_1^{(n)}} \to 0$.
Some of the elements of $\phi$ will diverge and some will go to zero.
This is a bad state to be in, so we will prefer to take $\pi$ to be unnormalized.

The one downside of using an unnormalized $\pi$ is that $\evec{}{0}$ will not be normalized
$\sum_i \evec{i}{0} \evec{i}{0} \pi_i = \sum_i \pi_i \neq 1$

\subsection{approximate pseudo-inverse of $K$}

Here I will write out the elements of the pseudo-inverse $K^{+}$ in explicit detail.
Where $K^+$ is constructed from the approximate eigenvectors.
\begin{equation}
  K^{+}_{ij} = \sum_{n=1}^{N-1} \lambda_n^{-1} \evec{i}{n} \evec{j}{n} \pi_j
\end{equation}
\begin{equation}
  K^{+}_{ij} = \sum_{n=1}^{N-1} \lambda_n^{-1} A_n^2 \pi_j 
  \begin{cases}
    \pi_{S_1^{(n)}}^{-1} & i \in S^{(n)}_1  j \in S^{(n)}_1 \\
  - \pi_{S_2^{(n)}}^{-1} & i \in S^{(n)}_1 j \in S^{(n)}_2 \\
  - \pi_{S_2^{(n)}}^{-1} & i \in S^{(n)}_2 j \in S^{(n)}_1 \\
    \frac{\pi_{S_1^{(n)}}}{\pi_{S_2^{(n)}}^2} & i \in S^{(n)}_2 j \in S^{(n)}_2 \\
  0 & \text{else}
  \end{cases}
\end{equation}
Again, if $i^*$ is an absorbing state then $K^+$ will be well behaved except possibly if
$j = i^*$.  In this case $\pi_j  \to \infty$.   But then, if $j \in S_2^{(n)}$ there is always at least one
term of $\pi_{S_2^{(n)}}^{-1}$ to balence it.  If $j \notin S_2^{(n)}$ the term will be $0$ because 
$j$ cannot be in $S_1^{(n)}$.  So again, all elements of $K^{+}$ are well behaved,
either finite or zero if $i^*$ is an absorbing state.
$K^+$ has `units' of $[\pi]^0$ so it doesn't actually matter if $\pi$ is normalized or not.  
All apperances of $P$ will cancel.


\subsection{approximate conditioned submatrix}
if we remove index $i^{*}$ then the conditioned submatrix is computed as
\begin{equation}
M_{ij} = \sum_{k} K^{+[i^{*}]}_{ik} \subK{}_{kj}  = 
\sum_{k \ne i^{*}} \sum_n \lambda_n^{-1} \evec{i}{n} \evec{k}{n} \pi_k K_{kj}
\end{equation}
Our approximation to $\phi^{(n)}$ can be written in a convenient way as
\begin{equation}
\evec{i}{n} = A_n
\begin{cases}
 \sum_{j \in S^{(n)}_2} \pi_{j} & i \in S^{(n)}_1 \\
-\sum_{j \in S^{(n)}_1} \pi_{j} & i \in S^{(n)}_2 \\
0 & \text{else}
\end{cases}
\end{equation}
where $A_n$ is a normalization factor.  Plugging in $\evec{k}{n}$ we get ({\color{red} ignoring $i^{*}$ for the time being})
\begin{equation}
M_{ij} = \sum_{k} K^{+}_{ik} K_{kj}  = 
\sum_n \lambda_n^{-1} \evec{i}{n} \left\{ 
  \sum_{k \in S_1^{(n)}} 
  \sum_{l \in S_2^{(n)}} 
  \pi_l
  \pi_k K_{kj}
  -
  \sum_{k \in S_2^{(n)}} 
  \sum_{l \in S_1^{(n)}} 
  \pi_l
  \pi_k K_{kj}
\right\}
\end{equation}
Using the following definition for the rate matrix 
\begin{equation}
K_{ij} = P
\begin{cases}
 \pi_{ij} / \pi_i & i \ne j \\
-\sum_{k \ne j} \pi_{jk} / \pi_j & i = j
\end{cases}
\end{equation}
where $\pi_{ij} = \exp (-\beta E_{ij})$, and P is the normalization for the $\Pi$, e.g. ($\pi_i = P \exp (-\beta E_i)$).
The equation for $M_{ij}$ can be re-written  ({\color{red} ignoring $A_n$ and $P$ for the time being})
\begin{align}
M_{ij} = 
\sum_n \lambda_n^{-1} \evec{i}{n} 
& \left\{ 
  \sum_{l \in S_2^{(n)}}   \pi_l
  \left[
  \sum_{\substack{k \in S_1^{(n)} \\ k \ne j}} 
  \pi_{kj}
   + \delta_{j \in S_1^{(n)}}
   \pi_j K_{jj}
  \right]
  \right. \\ & \left. 
  {} - \sum_{l \in S_1^{(n)}} \pi_l
  \left[
  \sum_{\substack{k \in S_2^{(n)} \\ k \ne j}} 
  \pi_{kj}
   + \delta_{j \in S_2^{(n)}}
   \pi_j K_{jj}
  \right]
\right\}
\end{align}
Which resolves to
\begin{align}
M_{ij} = 
\sum_n \lambda_n^{-1} \evec{i}{n} 
& \left\{ 
  \sum_{l \in S_2^{(n)}}   \pi_l
  \left[
  \sum_{\substack{k \in S_1^{(n)} \\ k \ne j}} 
  \pi_{kj}
   - \delta_{j \in S_1^{(n)}}
   \sum_{p \ne j} \pi_{jp}
  \right]
  \right. \\ & \left. 
  {} - \sum_{l \in S_1^{(n)}} \pi_l
  \left[
  \sum_{\substack{k \in S_2^{(n)} \\ k \ne j}} 
  \pi_{kj}
   - \delta_{j \in S_2^{(n)}}
   \sum_{p \ne j} \pi_{jp}
  \right]
\right\}
\end{align}
Breaking it up into the different cases, we find
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
\sum_{l \in S_2^{(n)}} \pi_l
  \left[
  \sum_{\substack{k \in S_1^{(n)} \\ k \ne j}} 
  \pi_{kj}
   - \sum_{p \ne j} \pi_{jp} 
  \right]
  - \sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
 & j \in S_1^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
-  \sum_{l \in S_1^{(n)}} \pi_l 
\left[
  \sum_{\substack{k \in S_2^{(n)} \\ k \ne j}} 
  \pi_{kj}
   - \sum_{p \ne j} \pi_{jp} 
  \right]
& j \in S_2^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}
Since $\pi_{ij} == \pi_{ji}$ we can simplify the terms in the bracket above
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
\sum_{l \in S_2^{(n)}} \pi_l
  \left[-
  \sum_{k \notin S_1^{(n)}} 
  \pi_{kj}
  \right]
  - \sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
 & j \in S_1^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
-  \sum_{l \in S_1^{(n)}} \pi_l 
\left[-
  \sum_{k \notin S_2^{(n)}} 
  \pi_{kj}
  \right]
& j \in S_2^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}
%\begin{equation}
%M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
%\begin{cases}
%- \sum_{l \in S_2^{(n)}} 
%   \sum_{k \notin S_1^{(n)}} 
%  \pi_l \pi_{kj}
%  - \sum_{l \in S_1^{(n)}} \sum_{k \in S_2^{(n)}}  \pi_l \pi_{kj}
% & j \in S_1^{(n)} \\
%\sum_{l \in S_2^{(n)}} \sum_{k \in S_1^{(n)}}  \pi_l \pi_{kj}
%+  \sum_{l \in S_1^{(n)}} 
%   \sum_{k \notin S_2^{(n)}} 
%  \pi_l \pi_{kj}
%& j \in S_2^{(n)} \\
%\sum_{l \in S_2^{(n)}} \sum_{k \in S_1^{(n)}}  \pi_l \pi_{kj}
%- 
%\sum_{l \in S_1^{(n)}} \sum_{k \in S_2^{(n)}} \pi_l  \pi_{kj}
%& \text{else}
%\end{cases}
%\end{equation}
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
-\sum_{l \in S_2^{(n)}} \pi_l
  \left[
  \sum_{k \in S_2^{(n)}} 
  \pi_{kj}
  +
  \sum_{k \notin S_1^{(n)} \cup S_2^{(n)}} 
  \pi_{kj}
  \right]
  - \sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
 & j \in S_1^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
+  \sum_{l \in S_1^{(n)}} \pi_l 
\left[
  \sum_{k \in S_1^{(n)}} 
  \pi_{kj}
  +
  \sum_{k \notin S_1^{(n)} \cup S_2^{(n)}} 
  \pi_{kj}
  \right]
& j \in S_2^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}
After rearranging the sums, this can be written
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
-\sum_{l \in S_1^{(n)} \cup S_2^{(n)}} \pi_l
\sum_{k \in S_2^{(n)}}  \pi_{kj}
-\sum_{l \in S_2^{(n)}} \pi_l
\sum_{k \notin S_1^{(n)} \cup S_2^{(n)}} \pi_{kj}
 & j \in S_1^{(n)} \\
\sum_{l \in S_1^{(n)} \cup S_2^{(n)}} \pi_l
\sum_{k \in S_1^{(n)}}  \pi_{kj}
+ \sum_{l \in S_1^{(n)}} \pi_l
\sum_{k \notin S_1^{(n)} \cup S_2^{(n)}} \pi_{kj}
& j \in S_2^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}
To simplify notation, define $S^{(n)} = S_1^{(n)} \cup S_2^{(n)}$
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
-\sum_{l \in S^{(n)}} \pi_l
\sum_{k \in S_2^{(n)}}  \pi_{kj}
-\sum_{l \in S_2^{(n)}} \pi_l
\sum_{k \notin S^{(n)}} \pi_{kj}
 & j \in S_1^{(n)} \\
\sum_{l \in S^{(n)}} \pi_l
\sum_{k \in S_1^{(n)}}  \pi_{kj}
+ \sum_{l \in S_1^{(n)}} \pi_l
\sum_{k \notin S^{(n)}} \pi_{kj}
& j \in S_2^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}
In order to further simplify things we now define $\pi_{S} = \sum_{i \in S} \pi_i$
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
-\pi_{S^{(n)}}
\sum_{k \in S_2^{(n)}}  \pi_{kj}
-\pi_{S_2^{(n)}}
\sum_{k \notin S^{(n)}} \pi_{kj}
 & j \in S_1^{(n)} \\
\pi_{S^{(n)}}
\sum_{k \in S_1^{(n)}}  \pi_{kj}
+ \pi_{S_1^{(n)}}
\sum_{k \notin S^{(n)}} \pi_{kj}
& j \in S_2^{(n)} \\
\pi_{S_2^{(n)}} \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\pi_{S_1^{(n)}} \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}
The remaining sums over $k$ are over distinct subsets, so unless $\pi_{lj} = \pi_{kj}$ for $k\ne l$ there will be no more canceling of terms.

To proceed further we note that the $S$ sets have the property that $S_1^{(m)}$ and  $S_2^{(m)}$ are completely contained
in $S_1^{(n)}$ or $S_2^{(n)}$ or neither for all $m > n$
\begin{equation}
S_1^{(m)} \cup S_2^{(m)} \subseteq 
\begin{cases}
S_1^{(n)} & \text{or} \\
S_2^{(n)} & \text{or} \\
\text{neither} & {}
\end{cases}
\end{equation}
It is this property that leads to orthogonality between the eigenvectors.

(new thought) But the eigenvalues are all different, so in order to get exact cancellation we need to use them.  We first write 
the eigenvalues in terms of $\pi_{ij}$
\begin{equation}
\lambda_n = \pi_{p_n q_n}
\end{equation}
which has the property that $p_n \in S_1^{(n)}$ and $q_n \in S_2^{(n)}$
\begin{equation}
M_{ij} = \sum_n \pi_{p_n q_n}^{-1} 
\left( \pi_{S_2^{(n)}} \delta \left(i \in S_1^{(n)}\right) 
+ \pi_{S_1^{(n)}} \delta \left(i  \in S_2^{(n)}\right) 
\right)
\begin{cases}
-\pi_{S^{(n)}}
\sum_{k \in S_2^{(n)}}  \pi_{kj}
-\pi_{S_2^{(n)}}
\sum_{k \notin S^{(n)}} \pi_{kj}
 & j \in S_1^{(n)} \\
\pi_{S^{(n)}}
\sum_{k \in S_1^{(n)}}  \pi_{kj}
+ \pi_{S_1^{(n)}}
\sum_{k \notin S^{(n)}} \pi_{kj}
& j \in S_2^{(n)} \\
\pi_{S_2^{(n)}} \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\pi_{S_1^{(n)}} \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}

\section{email from eric 7/17/2014}

I made a few checks - I’ll write everything componentwise for clarity.

Let $K_{i,j} , i,j=1,\ldots, N$ denote the entries of a generator that satisfies detailed balance wrt the distribution $\pi_i$, that is
\begin{equation}
\pi_i K_{i,j} = \pi_j K_{j,i} 		\text{ \quad   for all \quad    }  i,j
\end{equation}
and $\phi_i^{(n)}$, $\lambda_n$ with $n=0$, $\ldots, N-1$ be the eigenvector/eigenvalues pairs of $k_{i,j}$, i.e. the solutions to
\begin{equation}
\sum_j K_{i,j} \phi_j^{(n)} = \lambda_n \phi_i^{(n)}
\end{equation}
normalized such that
\begin{equation}
\sum_{i} \phi_i^{(n)} \phi_i^{(m)} \pi_i = \delta_{n,m}
\end{equation}
and ordered such that $\lambda_0 = 0 > \lambda_1 \ge \lambda_2 \ge \cdots (with \phi_i^{(0)} = 1)$. 
Then
\begin{equation}
\sum_{n=0}^{N-1} \phi_i^{(n)} \phi_j^{(n)} \pi_j = \sum_{n=0}^{N-1} \phi_j^{(n)} \phi_i^{(n)} \pi_i = \delta_{i,j} 
\end{equation}
and
\begin{equation}
K_{i,j} = \sum_{n=0}^{N-1} \lambda_n \phi_i^{(n)} \phi_j^{(n)} \pi_j = \sum_{n=1}^{N-1} \lambda_n \phi_i^{(n)} \phi_j^{(n)} \pi_j
\end{equation}
Furthermore, if we define
\begin{equation}
K^+_{i,j} = \sum_{n=1}^{N-1} \lambda_n^{-1} \phi_i^{(n)} \phi_j^{(n)} \pi_j
\end{equation}
then
\begin{equation}
\sum_k K_{i,k} K^+_{k,j} = \sum_k K^+_{i,k} K_{k,j} = \delta_{i,j} - \pi_j 
\end{equation}
These relations are illustrated in the matlab code attached below. They should hold both for the true spectral decomposition of K and for the approximated one (since it also define a generator). Can you check?

Next I’ll check the formulas for the MFPT.



\end{document}
