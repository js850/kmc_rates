\documentclass[a4paper]{article}

%\usepackage[numbers]{natbib}
\usepackage[usenames]{color} %for colored text
\usepackage[top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}


%\bibliographystyle{naturemagurl}
%\bibliographystyle{plainnat}

\newcommand{\evec}[2]{\phi^{(#2)}_{#1}}
\newcommand{\subK}{K^{[i^{*}]}}

\title{Notes on Minimum Spanning Tree spectral decomposition and preconditioning}
\author{Jacob Stevenson}
\date{\today}

\begin{document}
\maketitle

\section{notes on NGT}
Proof of NGT, from \cite{wales.2009}.  We would like to know how the transition probabilities and waiting times
are updated when node $x$ is removed.  The transition probability $P_{uv}$ is the probability that a markov chain 
transitions directly from u to v.  When we remove node $x$ we must sum over all possible paths that go through $x$.
As the nodes have self-transition probabilities, this is simply the sum over how many time steps the chain spends in $x$
\begin{equation}
P'_{uv} = P_{uv} + P_{ux} P_{xv} \sum_{m=0}^{\infty} P_{xx} = P_{uv} + \frac{ P_{ux} P_{xv} }{ 1 - P_{xx} }
\end{equation}
Computing how the waiting time is updated is only slightly more involved.  The waiting time $\tau_u$ is the average time to transition
from $u$ to one if it's neighbours.  After removing node $x$, $x$ will no longer be one of the neighbors of $u$, but we must sum
over all paths through x to a neighbour
\begin{equation}
\tau'_u = \sum_{v \ne x} \left\{ 
P_{uv} \tau_u + P_{ux} P_{xv} \left[ 
(\tau_u + \tau_x) + \sum_{m=1}^{\infty} (\tau_u + (m+1) \tau_x) P_{xx}^m
\right]
\right\}
\end{equation}
After some algebra, and resolving the sums ($\sum_{m=0}^{\infty} p^m = 1/(1-p)$ and $\sum_{m=0}^{\infty} m p^m = p/(1-p)^2$), this becomes
\begin{equation}
\tau'_u = \sum_{v \ne x} \left\{ 
P_{uv} \tau_u + P_{ux} P_{xv} \left[ 
\frac{\tau_u}{1-P_{xx}} + \frac{\tau_x}{(1-P_{xx})^2}
\right]
\right\}
\end{equation}
Finally we use $\sum_{v\ne x} P_{uv} = 1-P_{ux}$ and $\sum_{v\ne x} P_{xv} = 1-P_{xx}$ to conclude that
\begin{equation}
\tau'_u = \tau_u + \frac{ P_{ux} \tau_x}{1-P_{xx}}
\end{equation}


\section{eigenvalues and vectors}

The eigenvalues $\lambda_n$ and eigenvectors $\phi^{(n)}_i$ orthonormality

\begin{equation}
\sum_i \phi^{(n)}_i \pi_i \phi^{(m)}_i = \delta_{nm}
\end{equation}

They also satisfy the eigenvector equation
\begin{equation}
\sum_i K_{ij} \evec{i}{n}  = \lambda_n \evec{j}{n}
\end{equation}

This can be written more compactly as

\begin{align}
\Phi^{T} \Pi \Phi = I \\
K \Phi = \Phi \Lambda
\end{align}
where $\Pi$ and $\Lambda$ are diagonal matrices with $\pi_i$ and $\lambda_i$ along the diagonals.
The inverse of $\Phi$ is $\Phi^{T} \Pi$.
We can write K from the spectral decomposition
\begin{align}
K_{ij} &= \sum_n \lambda_n \evec{i}{n} \evec{j}{n} \pi_j \\
K &= \Phi \Lambda \Phi^T \Pi
\end{align}
The pseudo-inverse of $K$ can be written as
\begin{equation}
K^{+} = \Phi \Lambda^{-1} \Phi^T \Pi
\end{equation}
This is not the real inverse because the first eigenvector is zero.  But it does satisfy 
\begin{align}
K K^{+} K &= K \\
K^{+} K K^{+} &= K^{+}
\end{align}
and maybe (but I haven't checked it)
\begin{align}
(K K^{+})^T &= K K^{+} \\
(K^{+} K)^T &= K^{+} K
\end{align}

\section{submatrix spectrum}
We ultimately want to construct a preconditioner for a submatrix of $K$ denoted $\subK{}$ which is $K$
with row and column $i^{*}$ deleted.  This matrix is used to calculate mean first passage times from all the nodes
into $i^{*}$ via
\begin{equation}
\subK{} \tau_{mfpt} = (1, \dots, 1)^T
\end{equation}
The preconditioner will be an approximation to the inverse of $\subK{}$ which we will hopefully construct from 
the spectrum of $K$.

We will proceed by altering the original problem so that the energy of the node to be removed $E_{i^{*}}$
is much less than all other energies in the problem.  The spectrum of $K$ is then computed.

We will try to show that the eigenvectors of $\subK{}$ are the same as the eigenvectors of $K$ with 
the $i^{*}$th element removed. The eigenvector corresponding to the zero eigenvalue is discarded.

We are taking the limit $E_{i^{*}} \to -\infty$, so we will define a small parameter $p = \exp{\beta E_{i^{*}}}$.
Numerically it appears that
\begin{align}
\lambda_{i} &\sim 1 \\
\evec{i^{*}}{n} &\sim p^{1/2} \\
\evec{i\ne i^{*}}{n} &\sim p^{-1/2} \text{  (except, possibly, for n=1)} \\
\pi_{i^{*}} &\sim 1 \\
\pi_{i\ne i^{*}} &\sim p \\
K_{i^{*} j} &\sim p \\
K_{i\ne i^{*}, j} &\sim 1
\end{align}

First show that they are orthonormal.
\begin{align}
\sum_{i\ne i^{*}} \evec{i}{n} \pi_i \evec{i}{m} &= \delta_{nm} - \evec{i^{*}}{n} \evec{i^{*}}{m} \pi_{i^{*}} \\
& \sim \delta_{nm} - p
\end{align}
Thus they are orthonormal for small enough p.  We now show that they satisfy the eigenvector equation
\begin{align}
\sum_{j\ne i^{*}} K_{ij} \evec{j}{n}  &=
\lambda_n \evec{i}{n} - K_{i i^{*}} \evec{i^{*}}{n} \\
& \sim \lambda_n \evec{i}{n} - p^{1/2}
\end{align}
Similarly, it can be shown that
\begin{align}
\sum_{i\ne i^{*}} 
\sum_{j\ne i^{*}} 
\evec{i}{m} \pi_{i}
K_{ij} \evec{j}{n} 
&= \lambda_n \delta_{nm} + \evec{i^{*}}{n} \evec{i^{*}}{m} \pi_{i^{*}} (K_{i^{*} i^{*}} - \lambda_n - \lambda_m)
\\
& \sim \lambda_n \evec{i}{n} + p (p - 1)
\end{align}

\subsection{conditioned submatrix}
if we remove index $i^{*}$ then the conditioned submatrix is computed as
\begin{equation}
M_{ij} = \sum_{k} K^{+[i^{*}]}_{ik} \subK{}_{kj}  = 
\sum_{k \ne i^{*}} \sum_n \lambda_n^{-1} \evec{i}{n} \evec{k}{n} \pi_k K_{kj}
\end{equation}
$\phi^{(n)}$ can be written in a convenient way as
\begin{equation}
\evec{i}{n} = A_n
\begin{cases}
 \sum_{j \in S^{(n)}_2} \pi_{j} & i \in S^{(n)}_1 \\
-\sum_{j \in S^{(n)}_1} \pi_{j} & i \in S^{(n)}_2 \\
0 & \text{else}
\end{cases}
\end{equation}
where $A_n$ is a normalization factor.  Plugging in $\evec{k}{n}$ we get ({\color{red} ignoring $i^{*}$})
\begin{equation}
M_{ij} = \sum_{k} K^{+}_{ik} K_{kj}  = 
\sum_n \lambda_n^{-1} \evec{i}{n} \left\{ 
  \sum_{k \in S_1^{(n)}} 
  \sum_{l \in S_2^{(n)}} 
  \pi_l
  \pi_k K_{kj}
  -
  \sum_{k \in S_2^{(n)}} 
  \sum_{l \in S_1^{(n)}} 
  \pi_l
  \pi_k K_{kj}
\right\}
\end{equation}
Using the following definition for the rate matrix 
\begin{equation}
K_{ij} = P^{-1}
\begin{cases}
 \pi_{ij} / \pi_i & i \ne j \\
-\sum_{k \ne j} \pi_{jk} / \pi_j & i = j
\end{cases}
\end{equation}
where $\pi_{ij} = \exp (-\beta E_{ij})$, and P is the normalization for the $\Pi$, e.g. ($\pi_i = P \exp (-\beta E_i)$).
The equation for $M_{ij}$ can be re-written
\begin{align}
M_{ij} = 
\sum_n \lambda_n^{-1} \evec{i}{n} 
& \left\{ 
  \sum_{l \in S_2^{(n)}}   \pi_l
  \left[
  \sum_{\substack{k \in S_1^{(n)} \\ k \ne j}} 
  \pi_{kj}
   + \delta_{j \in S_1^{(n)}}
   \pi_j K_{jj}
  \right]
  \right. \\ & \left. 
  {} - \sum_{l \in S_1^{(n)}} \pi_l
  \left[
  \sum_{\substack{k \in S_2^{(n)} \\ k \ne j}} 
  \pi_{kj}
   + \delta_{j \in S_2^{(n)}}
   \pi_j K_{jj}
  \right]
\right\}
\end{align}
Which resolves to
\begin{align}
M_{ij} = 
\sum_n \lambda_n^{-1} \evec{i}{n} 
& \left\{ 
  \sum_{l \in S_2^{(n)}}   \pi_l
  \left[
  \sum_{\substack{k \in S_1^{(n)} \\ k \ne j}} 
  \pi_{kj}
   - \delta_{j \in S_1^{(n)}}
   \sum_{p \ne j} \pi_{jp}
  \right]
  \right. \\ & \left. 
  {} - \sum_{l \in S_1^{(n)}} \pi_l
  \left[
  \sum_{\substack{k \in S_2^{(n)} \\ k \ne j}} 
  \pi_{kj}
   - \delta_{j \in S_2^{(n)}}
   \sum_{p \ne j} \pi_{jp}
  \right]
\right\}
\end{align}
Breaking it up into the different cases, we find
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
\sum_{l \in S_2^{(n)}} \pi_l
  \left[
  \sum_{\substack{k \in S_1^{(n)} \\ k \ne j}} 
  \pi_{kj}
   - \sum_{p \ne j} \pi_{jp} 
  \right]
  - \sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
 & j \in S_1^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
-  \sum_{l \in S_1^{(n)}} \pi_l 
\left[
  \sum_{\substack{k \in S_2^{(n)} \\ k \ne j}} 
  \pi_{kj}
   - \sum_{p \ne j} \pi_{jp} 
  \right]
& j \in S_2^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}
Since $\pi_{ij} == \pi_{ji}$ we can simplify the terms in the bracket above
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
\sum_{l \in S_2^{(n)}} \pi_l
  \left[-
  \sum_{\substack{k \notin S_1^{(n)} \\ k \ne j}} 
  \pi_{kj}
  \right]
  - \sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
 & j \in S_1^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
-  \sum_{l \in S_1^{(n)}} \pi_l 
\left[-
  \sum_{\substack{k \notin S_2^{(n)} \\ k \ne j}} 
  \pi_{kj}
  \right]
& j \in S_2^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
- \sum_{l \in S_2^{(n)}} 
   \sum_{\substack{k \notin S_1^{(n)} \\ k \ne j}} 
  \pi_l \pi_{kj}
  - \sum_{l \in S_1^{(n)}} \sum_{k \in S_2^{(n)}}  \pi_l \pi_{kj}
 & j \in S_1^{(n)} \\
\sum_{l \in S_2^{(n)}} \sum_{k \in S_1^{(n)}}  \pi_l \pi_{kj}
+  \sum_{l \in S_1^{(n)}} 
   \sum_{\substack{k \notin S_2^{(n)} \\ k \ne j}} 
  \pi_l \pi_{kj}
& j \in S_2^{(n)} \\
\sum_{l \in S_2^{(n)}} \sum_{k \in S_1^{(n)}}  \pi_l \pi_{kj}
- 
\sum_{l \in S_1^{(n)}} \sum_{k \in S_2^{(n)}} \pi_l  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}
\begin{equation}
M_{ij} = \sum_n \lambda_n^{-1} \evec{i}{n} 
\begin{cases}
-\sum_{l \in S_2^{(n)}} \pi_l
  \left[
  \sum_{k \in S_2^{(n)}} 
  \pi_{kj}
  +
  \sum_{\substack{k \notin S_1^{(n)} \cup S_2^{(n)} \\ k \ne j}} 
  \pi_{kj}
  \right]
  - \sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
 & j \in S_1^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
+  \sum_{l \in S_1^{(n)}} \pi_l 
\left[
  \sum_{k \in S_1^{(n)}} 
  \pi_{kj}
  +
  \sum_{\substack{k \notin S_1^{(n)} \cup S_2^{(n)}  \\ k \ne j}} 
  \pi_{kj}
  \right]
& j \in S_2^{(n)} \\
\sum_{l \in S_2^{(n)}} \pi_l \sum_{k \in S_1^{(n)}}  \pi_{kj}
- 
\sum_{l \in S_1^{(n)}} \pi_l \sum_{k \in S_2^{(n)}}  \pi_{kj}
& \text{else}
\end{cases}
\end{equation}

\end{document}
